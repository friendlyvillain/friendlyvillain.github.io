---
title: Markov Decison Process (MDP)
author: jh
date: 2023-05-01 00:53:20 +0900
categories: [Machine Learning, Reinforcement Learning]
tags: [ML, RL, MDP, State-value function, Action-value function, Bellman equation]
math: true
mermaid: true
comments: true
---

## Markov Decision Process 

정의: 미래의 상태 (State) 와 보상 (Reward) 는 오직 현재의 State 와 Reward 에 의해 결정됨

### Agent-Environment Structure

- **Agent**: 
RL에서 환경을 학습 (learn)하고, 행동 (Action)을 결정 (decision making)

- **Environment**: 
Agent와 상호작용하는 외부의 환경

- **Set of state**: $$ s \in \mathcal{S} $$

- **Set of actions**: $$ A(s) \in \mathcal{A} $$

- **Set of rewards**: $$ r \in \mathcal{R} \subset \mathbb{R} $$

![agent-env-structure](/assets/img/posts/mdp/agent_env_structure.png){: width="500" height="200" }

- **Trajectory (Finte MDP case)**: $$ S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_{T} $$

- **Goal of Agent**: 주어진 State 에서 누적된 보상 (Accumulated reward: $$ \mathbb{E}[G_t] $$ )을 최대화하는 행동을 결정

$$ G_t = R_{t+1} + \gamma R_{t+2} + ... = R_{t+1} + \gamma G_{t+1} \ where \ 0 \leq \gamma \leq 1$$

### Dynamics of MDP

MDP의 Dynamics를 안다는 것은 Environment의 모든 State와 모든 Action에 대해, state transition probabilty를 아는 것과 동일한 의미
(일반적인 강화학습 문제에서는 MDP의 Dynamics를 알지 못함)

- **State transition probability**: 
$$ p(s', r | s, a) \doteq \mathcal{P}\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\} $$

$$ \rightarrow \sum_{s'}\sum_{r} p(s',r|s, a)=1 $$

- **Expected reward**:
$$ r(s, a) \doteq \mathbb{E}[R_t | S_{t-1}=s, A_{t-1}=a] = \sum_{r}\sum_{s'} r \mathcal{P}(s', r | s, a) $$

## Reference
[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)